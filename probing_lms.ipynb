{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Probing Language Models__\n",
    "\n",
    "This notebook serves as a start for your NLP2 assignment on probing Language Models. This notebook will become part of the contents that you will submit at the end, so make sure to keep your code (somewhat) clean :-)\n",
    "\n",
    "__note__: This assignment is not dependent on big fancy GPUs. I run all this stuff on my own 3 year old CPU, without any Colab hassle. So it's up to you to decide how you want to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Models\n",
    "\n",
    "For the Transformer models you are advised to make use of the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "Their library is well documented, and they provide great tools to easily load in pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the transformer model(s)\n",
    "#\n",
    "# Note that most transformer models use their own `tokenizer`, that should be loaded in as well.\n",
    "#\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the rnn model(s)\n",
    "#\n",
    "# The Gulordava LSTM model can be found here: \n",
    "# https://drive.google.com/file/d/19Lp3AM4NEPycp_IBgoHfLc_V456pmUom/view?usp=sharing\n",
    "# You can read more about this model in the original paper here: https://arxiv.org/pdf/1803.11138.pdf\n",
    "#\n",
    "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
    "# If you want to do more experiments with this model you could have a look at the original code here:\n",
    "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
    "#\n",
    "from collections import defaultdict\n",
    "from lstm.model import RNNModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model_location = 'state_dict.pt'  # <- point this to the location of the Gulordava .pt file\n",
    "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
    "lstm.load_state_dict(torch.load(model_location))\n",
    "\n",
    "\n",
    "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
    "with open('lstm/vocab.txt', encoding=\"utf8\") as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is a good idea that before you move on, you try to feed some text to your LMs; and check if everything works accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "For this assignment you will train your probes on __treebank__ corpora. A treebank is a corpus that has been *parsed*, and stored in a representation that allows the parse tree to be recovered. Next to a parse tree, treebanks also often contain information about part-of-speech tags, which is exactly what we are after now.\n",
    "\n",
    "The treebank you will use for now is part of the Universal Dependencies project. I provide a sample of this treebank as well, so you can test your setup on that before moving on to larger amounts of data.\n",
    "\n",
    "Make sure you accustom yourself to the format that is created by the `conllu` library that parses the treebank files before moving on. For example, make sure you understand how you can access the pos tag of a token, or how to cope with the tree structure that is formed using the `to_tree()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "\n",
    "\n",
    "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
    "# These are type hints that help you to understand what kind of argument and output is expected.\n",
    "def parse_corpus(filename: str) -> List[TokenList]:\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TokenList<Al, -, Zaman, :, American, forces, killed, Shaikh, Abdullah, al, -, Ani, ,, the, preacher, at, the, mosque, in, the, town, of, Qaim, ,, near, the, Syrian, border, .>, TokenList<[, This, killing, of, a, respected, cleric, will, be, causing, us, trouble, for, years, to, come, ., ]>, TokenList<DPA, :, Iraqi, authorities, announced, that, they, had, busted, up, 3, terrorist, cells, operating, in, Baghdad, .>, TokenList<Two, of, them, were, being, run, by, 2, officials, of, the, Ministry, of, the, Interior, !>, TokenList<The, MoI, in, Iraq, is, equivalent, to, the, US, FBI, ,, so, this, would, be, like, having, J., Edgar, Hoover, unwittingly, employ, at, a, high, level, members, of, the, Weathermen, bombers, back, in, the, 1960s, .>, TokenList<The, third, was, being, run, by, the, head, of, an, investment, firm, .>, TokenList<You, wonder, if, he, was, manipulating, the, market, with, his, bombing, targets, .>, TokenList<The, cells, were, operating, in, the, Ghazaliyah, and, al, -, Jihad, districts, of, the, capital, .>, TokenList<Although, the, announcement, was, probably, made, to, show, progress, in, identifying, and, breaking, up, terror, cells, ,, I, do, n't, find, the, news, that, the, Baathists, continue, to, penetrate, the, Iraqi, government, very, hopeful, .>, TokenList<It, reminds, me, too, much, of, the, ARVN, officers, who, were, secretly, working, for, the, other, side, in, Vietnam, .>]\n"
     ]
    }
   ],
   "source": [
    "ud_parses = parse_corpus(\"data/en_ewt-ud-train.conllu\")\n",
    "print(ud_parses[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generating Representations\n",
    "\n",
    "We now have our data all set, our models are running and we are good to go!\n",
    "\n",
    "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
    "\n",
    "There are a few things you should keep in mind here. Read these carefully, as these tips will save you a lot of time in your implementation.\n",
    "1. Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "\n",
    "2. Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. This is great for our setup! Otherwise it would have been quite challenging to distribute the representation of a subword over the 2 tokens it belongs to.\n",
    "\n",
    "\n",
    "3. **Important**: If you closely examine the provided treebank, you will notice that some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. <br /><br />\n",
    "Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. That is, the tokenizer uses a different token id for `\"man\"`, than it does for `\" man\"`: the former could be part of `\" woman\"`=`\" wo`\"+`\"man\"`, whereas the latter would be the used in case *man* occurs at the start of a word. The tokenizer for GPT-2 adds spaces at the start of a token (represented as a `Ä ` symbol). This means that you should keep track whether the previous token had the `SpaceAfter` attribute set to `'No'`: in case it did not, you should manually prepend a `\" \"` ahead of the token.\n",
    "\n",
    "\n",
    "4. The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. Make sure you keep the above points in mind though, when creating the LSTM representations. You might want to write separate functions for the LSTM, but that is up to you.\n",
    "\n",
    "\n",
    "5. The huggingface transformer models don't return the hidden state by default. To achieve this you can pass `output_hidden_states=True` to a model forward pass. The hidden states are then returned for all intermediate layers as well, the latest entry in this list corresponds to the top layer.\n",
    "\n",
    "\n",
    "6. **N.B.**: Make sure that when you run a sentence through your model, you do so within a `with torch.no_grad():` block, and that you have run `model.eval()` beforehand as well (to disable dropout).\n",
    "\n",
    "\n",
    "7. **N.B.**: Make sure to use a token's ``[\"form\"]`` attribute, and not the ``[\"lemma\"]``, as the latter will stem any relevant morphological information from the token. We don't want this, because we want to feed well-formed, grammatical sentences to our model.\n",
    "\n",
    "\n",
    "I would like to stress that if you feel hindered in any way by the simple code structure that is presented here, you are free to modify it :-) Just make sure it is clear to an outsider what you're doing, some helpful comments never hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FETCH SENTENCE REPRESENTATIONS\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus, representation_size)\n",
    "# Make sure you correctly average the subword representations that belong to 1 token!\n",
    "\n",
    "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer) -> Tensor:\n",
    "    representation_size = 768\n",
    "    out = []\n",
    "    \n",
    "    for sentence in tqdm(ud_parses):\n",
    "        j = 0\n",
    "        concat_dict = {}\n",
    "        token_list = []\n",
    "        space_after = False\n",
    "        for token in sentence:\n",
    "            \n",
    "            test_var = token[\"misc\"]\n",
    "            \n",
    "            if space_after:\n",
    "                token_return = tokenizer.encode(str(token), add_prefix_space=True)\n",
    "            else:\n",
    "                token_return = tokenizer.encode(str(token))\n",
    "            if test_var:\n",
    "                space_after = False\n",
    "            else:\n",
    "                space_after = True        \n",
    "            \n",
    "            len_i = len(token_return)\n",
    "            concat_dict[j] = [j+i for i in range(len_i)]\n",
    "            j += len_i\n",
    "            token_list += token_return\n",
    "            \n",
    "        token_list = torch.LongTensor(token_list)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out_sentence = model(input_ids = token_list, output_hidden_states=True)\n",
    "        out_sentence = out_sentence[\"hidden_states\"][-1].squeeze()  \n",
    "        \n",
    "        \n",
    "        out_sent = torch.zeros(len(sentence), representation_size)\n",
    "        for i, key in enumerate(concat_dict):\n",
    "            out_sent[i] = torch.mean(out_sentence[concat_dict[key]], axis=0)\n",
    "        out += out_sent\n",
    "        \n",
    "    out = torch.stack(out)\n",
    "\n",
    "    return out      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_sen_reps_lstm(ud_parses: List[TokenList], model, tokenizer) -> Tensor:\n",
    "    representation_size = 650\n",
    "    out = []\n",
    "    for sentence in ud_parses:\n",
    "        sent = torch.zeros((len(sentence), 1))\n",
    "        for i, token in enumerate(sentence):\n",
    "            sent[i] = tokenizer[str(token)]\n",
    "        sent = sent.long()\n",
    "        hidden = model.init_hidden(1)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out_sentence = model(sent, hidden)\n",
    "\n",
    "        out += out_sentence.squeeze()\n",
    "    out = torch.stack(out)\n",
    "    return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To validate your activation extraction procedure I have set up the following assertion function as a sanity check. It compares your representation against a pickled version of mine. \n",
    "\n",
    "For this I used `distilgpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.7228e-04, -2.4058e-03,  2.1826e-02,  ..., -5.8634e-04,\n",
      "          2.1452e-03, -2.1771e-01],\n",
      "        [ 4.5972e-03,  4.1278e-02, -8.6334e-02,  ...,  3.5224e-02,\n",
      "          1.8264e-03, -6.8585e-02],\n",
      "        [ 1.3671e-01, -4.9340e-03, -1.1839e-01,  ..., -6.4270e-03,\n",
      "          1.1818e-02, -1.2792e-02],\n",
      "        ...,\n",
      "        [ 1.3563e-01, -1.3524e-02,  1.3721e-01,  ...,  6.2993e-02,\n",
      "          1.2704e-01,  5.2278e-02],\n",
      "        [-5.9570e-01, -9.1277e-04,  1.8890e-02,  ..., -8.8383e-04,\n",
      "          1.6363e-01,  7.0265e-01],\n",
      "        [-1.6506e-05, -2.6296e-09, -3.4986e-05,  ..., -1.8230e-09,\n",
      "          1.3924e-04,  1.4111e-04]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1/1 [00:00<00:00, 10.48it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 29, 650), got [2, 1, 650]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 45>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mallclose(lstm_emb1, own_lstm_emb1, rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m, atol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m):\n\u001B[1;32m     42\u001B[0m         error_msg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlstm\u001B[39m\u001B[38;5;124m\"\u001B[39m, lstm_emb1, own_lstm_emb1, \u001B[38;5;28mlist\u001B[39m(vocab\u001B[38;5;241m.\u001B[39mkeys()))\n\u001B[0;32m---> 45\u001B[0m \u001B[43massert_sen_reps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlstm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36massert_sen_reps\u001B[0;34m(model, tokenizer, lstm, vocab)\u001B[0m\n\u001B[1;32m     29\u001B[0m corpus \u001B[38;5;241m=\u001B[39m parse_corpus(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/sample/en_ewt-ud-train.conllu\u001B[39m\u001B[38;5;124m'\u001B[39m)[:\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     31\u001B[0m own_distilgpt2_emb1 \u001B[38;5;241m=\u001B[39m fetch_sen_reps(corpus, model, tokenizer)\n\u001B[0;32m---> 32\u001B[0m own_lstm_emb1 \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_sen_reps_lstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlstm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m distilgpt2_emb1\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m own_distilgpt2_emb1\u001B[38;5;241m.\u001B[39mshape, \\\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDistilgpt2 shape mismatch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdistilgpt2_emb1\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (gold) vs. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mown_distilgpt2_emb1\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (yours)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m lstm_emb1\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m own_lstm_emb1\u001B[38;5;241m.\u001B[39mshape, \\\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLSTM shape mismatch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlstm_emb1\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (gold) vs. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mown_lstm_emb1\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (yours)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36mfetch_sen_reps_lstm\u001B[0;34m(ud_parses, model, tokenizer)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     11\u001B[0m         model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m---> 12\u001B[0m         out_sentence \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m out_sentence\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     15\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(out)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/lstm/model.py:58\u001B[0m, in \u001B[0;36mRNNModel.forward\u001B[0;34m(self, input, hidden, output_vocab_probs)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, hidden, output_vocab_probs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     57\u001B[0m     emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m---> 58\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43memb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m output_vocab_probs:\n\u001B[1;32m     61\u001B[0m         decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(output\u001B[38;5;241m.\u001B[39mview(output\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m*\u001B[39moutput\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m), output\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m)))\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:759\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    755\u001B[0m     \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m--> 759\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_forward_args\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    760\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    761\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    762\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:685\u001B[0m, in \u001B[0;36mLSTM.check_forward_args\u001B[0;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m,  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m    680\u001B[0m                        \u001B[38;5;28minput\u001B[39m: Tensor,\n\u001B[1;32m    681\u001B[0m                        hidden: Tuple[Tensor, Tensor],\n\u001B[1;32m    682\u001B[0m                        batch_sizes: Optional[Tensor],\n\u001B[1;32m    683\u001B[0m                        ):\n\u001B[1;32m    684\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[0;32m--> 685\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_hidden_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_expected_hidden_size\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m                           \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mExpected hidden[0] size \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m, got \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    687\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_cell_size(\u001B[38;5;28minput\u001B[39m, batch_sizes),\n\u001B[1;32m    688\u001B[0m                            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden[1] size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:226\u001B[0m, in \u001B[0;36mRNNBase.check_hidden_size\u001B[0;34m(self, hx, expected_hidden_size, msg)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_hidden_size\u001B[39m(\u001B[38;5;28mself\u001B[39m, hx: Tensor, expected_hidden_size: Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m    224\u001B[0m                       msg: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected hidden size \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hx\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m expected_hidden_size:\n\u001B[0;32m--> 226\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(expected_hidden_size, \u001B[38;5;28mlist\u001B[39m(hx\u001B[38;5;241m.\u001B[39msize())))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected hidden[0] size (2, 29, 650), got [2, 1, 650]"
     ]
    }
   ],
   "source": [
    "def error_msg(model_name, gold_embs, embs, i2w):\n",
    "    with open(f'{model_name}_tokens1.pickle', 'rb') as f:\n",
    "        sen_tokens = pickle.load(f)\n",
    "        \n",
    "    diff = torch.abs(embs - gold_embs)\n",
    "    max_diff = torch.max(diff)\n",
    "    avg_diff = torch.mean(diff)\n",
    "    \n",
    "    print(f\"{model_name} embeddings don't match!\")\n",
    "    print(f\"Max diff.: {max_diff:.4f}\\nMean diff. {avg_diff:.4f}\")\n",
    "\n",
    "    print(\"\\nCheck if your tokenization matches with the original tokenization:\")\n",
    "    for idx in sen_tokens.squeeze():\n",
    "        if isinstance(i2w, list):\n",
    "            token = i2w[idx]\n",
    "        else:\n",
    "            token = i2w.convert_ids_to_tokens(idx.item())\n",
    "        print(f\"{idx:<6} {token}\")\n",
    "\n",
    "\n",
    "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
    "    with open('distilgpt2_emb1.pickle', 'rb') as f:\n",
    "        distilgpt2_emb1 = pickle.load(f)\n",
    "        \n",
    "    with open('lstm_emb1.pickle', 'rb') as f:\n",
    "        lstm_emb1 = pickle.load(f)\n",
    "        print(lstm_emb1)\n",
    "    \n",
    "    corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')[:1]\n",
    "    \n",
    "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)\n",
    "    own_lstm_emb1 = fetch_sen_reps_lstm(corpus, lstm, vocab)\n",
    "    \n",
    "    assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape, \\\n",
    "        f\"Distilgpt2 shape mismatch: {distilgpt2_emb1.shape} (gold) vs. {own_distilgpt2_emb1.shape} (yours)\"\n",
    "    assert lstm_emb1.shape == own_lstm_emb1.shape, \\\n",
    "        f\"LSTM shape mismatch: {lstm_emb1.shape} (gold) vs. {own_lstm_emb1.shape} (yours)\"\n",
    "\n",
    "    if not torch.allclose(distilgpt2_emb1, own_distilgpt2_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"distilgpt2\", distilgpt2_emb1, own_distilgpt2_emb1, tokenizer)\n",
    "    if not torch.allclose(lstm_emb1, own_lstm_emb1, rtol=1e-3, atol=1e-3):\n",
    "        error_msg(\"lstm\", lstm_emb1, own_lstm_emb1, list(vocab.keys()))\n",
    "\n",
    "\n",
    "assert_sen_reps(model, tokenizer, lstm, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we should define a function that extracts the corresponding POS labels for each activation, which we do based on the **``\"upostag\"``** attribute of a token (so not the ``xpostag`` attribute). These labels will be transformed to a tensor containing the label index for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FETCH POS LABELS\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
    "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
    "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab=None) -> Tensor:\n",
    "    pos_tags = list()\n",
    "    for sentence in ud_parses:\n",
    "        for token in sentence:\n",
    "\n",
    "            pos_tags.append(token[\"upostag\"])\n",
    "\n",
    "    if pos_vocab:\n",
    "        targets = pos_vocab.fit_transform(pos_tags)\n",
    "        pos_tags = torch.as_tensor(targets)\n",
    "    else:\n",
    "        pos_vocab = preprocessing.LabelEncoder()\n",
    "        targets = pos_vocab.fit_transform(pos_tags)\n",
    "        pos_tags = torch.as_tensor(targets)\n",
    "  \n",
    "    return pos_tags, pos_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n"
     ]
    }
   ],
   "source": [
    "pos_tags = fetch_pos_tags(ud_parses)\n",
    "print(torch.unique(pos_tags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12543 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m w2i \u001B[38;5;241m=\u001B[39m vocab  \u001B[38;5;66;03m# or `vocab`\u001B[39;00m\n\u001B[1;32m     17\u001B[0m use_sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m train_x, train_y, train_vocab \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msample\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43muse_sample\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_ewt-ud-train.conllu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43mw2i\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m dev_x, dev_y, _ \u001B[38;5;241m=\u001B[39m create_data(\n\u001B[1;32m     26\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_sample \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_ewt-ud-dev.conllu\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     27\u001B[0m     lm, \n\u001B[1;32m     28\u001B[0m     w2i,\n\u001B[1;32m     29\u001B[0m     pos_vocab\u001B[38;5;241m=\u001B[39mtrain_vocab\n\u001B[1;32m     30\u001B[0m )\n\u001B[1;32m     32\u001B[0m test_x, test_y, _ \u001B[38;5;241m=\u001B[39m create_data(\n\u001B[1;32m     33\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msample\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_sample \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_ewt-ud-test.conllu\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     34\u001B[0m     lm,\n\u001B[1;32m     35\u001B[0m     w2i,\n\u001B[1;32m     36\u001B[0m     pos_vocab\u001B[38;5;241m=\u001B[39mtrain_vocab\n\u001B[1;32m     37\u001B[0m )\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mcreate_data\u001B[0;34m(filename, lm, w2i, pos_vocab)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_data\u001B[39m(filename: \u001B[38;5;28mstr\u001B[39m, lm, w2i, pos_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m      7\u001B[0m     ud_parses \u001B[38;5;241m=\u001B[39m parse_corpus(filename)\n\u001B[0;32m----> 9\u001B[0m     sen_reps \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_sen_reps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mud_parses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw2i\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     pos_tags, pos_vocab \u001B[38;5;241m=\u001B[39m fetch_pos_tags(ud_parses, pos_vocab\u001B[38;5;241m=\u001B[39mpos_vocab)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sen_reps, pos_tags, pos_vocab\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mfetch_sen_reps\u001B[0;34m(ud_parses, model, tokenizer)\u001B[0m\n\u001B[1;32m     24\u001B[0m     token_return \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;28mstr\u001B[39m(token), add_prefix_space\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 26\u001B[0m     token_return \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m(\u001B[38;5;28mstr\u001B[39m(token))\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m test_var:\n\u001B[1;32m     28\u001B[0m     space_after \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'collections.defaultdict' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
    "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
    "\n",
    "def create_data(filename: str, lm, w2i, pos_vocab=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    \n",
    "    sen_reps = fetch_sen_reps(ud_parses, lm, w2i)\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses, pos_vocab=pos_vocab)\n",
    "    \n",
    "    return sen_reps, pos_tags, pos_vocab\n",
    "\n",
    "\n",
    "lm = lstm  # or `lstm`\n",
    "w2i = vocab  # or `vocab`\n",
    "use_sample = False\n",
    "\n",
    "train_x, train_y, train_vocab = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "    lm, \n",
    "    w2i\n",
    ")\n",
    "\n",
    "dev_x, dev_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "    lm, \n",
    "    w2i,\n",
    "    pos_vocab=train_vocab\n",
    ")\n",
    "\n",
    "test_x, test_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "    lm,\n",
    "    w2i,\n",
    "    pos_vocab=train_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Diagnostic Classification\n",
    "\n",
    "We now have our models, our data, _and_ our representations all set! Hurray, well done. We can finally move onto the cool stuff, i.e. training the diagnostic classifiers (DCs).\n",
    "\n",
    "DCs are simple in their complexity on purpose. To read more about why this is the case you could already have a look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2).\n",
    "\n",
    "A simple linear classifier will suffice for now, don't bother with adding fancy non-linearities to it.\n",
    "\n",
    "I am personally a fan of the `skorch` library, that provides `sklearn`-like functionalities for training `torch` models, but you are free to train your dc using whatever method you prefer.\n",
    "\n",
    "As this is an Artificial Intelligence master and you have all done ML1 + DL, I expect you to use your train/dev/test splits correctly ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "#import skorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(650,17)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "def pos_probe_linear(train_x, train_y, dev_x, dev_y):\n",
    "    lr = 10e-4\n",
    "    batch_size = 256\n",
    "    epochs = 10000\n",
    "    classifier = LinearClassifier()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "    all_loss = []\n",
    "\n",
    "    accuracy = torchmetrics.Accuracy()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            x_batch, y_batch = train_x[i:i+batch_size], train_y[i:i+batch_size]\n",
    "\n",
    "            output = classifier(x_batch)\n",
    "\n",
    "            loss = criterion(output, y_batch)\n",
    "            all_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 1000 == 0 :\n",
    "            classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                output = classifier(train_x)\n",
    "                output_dev = classifier(dev_x)\n",
    "                dev_acc = accuracy(output_dev, dev_y)\n",
    "                train_acc = accuracy(output, train_y)\n",
    "                print(f\"Validation accuracy on batch {epoch}: {dev_acc}\")\n",
    "                print(f\"Training accuracy on batch {epoch}: {train_acc}\")\n",
    "        \n",
    "    torch.save(classifier, \"linear_POS_LSTM.pt\")\n",
    "        \n",
    "    return classifier\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m classifier \u001B[38;5;241m=\u001B[39m pos_probe_linear(\u001B[43mtrain_x\u001B[49m, train_y, dev_x, dev_y)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = pos_probe_linear(train_x, train_y, dev_x, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "output_test = classifier(test_x)\n",
    "test_acc = accuracy(output_test, test_y)\n",
    "print(\"The test accuracy is\",test_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Non-Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "#import skorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class nonLinearClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(768,512)\n",
    "    self.linear2 = nn.Linear(512,17)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = self.sigmoid(x)\n",
    "    x = self.linear2(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    #x = F.softmax(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def pos_probe_non_linear(train_x, train_y, dev_x, dev_y):\n",
    "    non_linear_classifier = nonLinearClassifier()\n",
    "    lr = 10e-4\n",
    "    batch_size = 256\n",
    "    epochs = 10000\n",
    "\n",
    "    #doubling the criterions\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(non_linear_classifier.parameters(), lr=lr)\n",
    "\n",
    "    all_loss = []\n",
    "\n",
    "    accuracy = torchmetrics.Accuracy()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            x_batch, y_batch = train_x[i:i+batch_size], train_y[i:i+batch_size]\n",
    "\n",
    "            output = non_linear_classifier(x_batch)\n",
    "\n",
    "            loss = criterion(output, y_batch)\n",
    "            #print(loss)\n",
    "            all_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 1000 == 0 :\n",
    "            non_linear_classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                output_dev = non_linear_classifier(dev_x)\n",
    "                output = non_linear_classifier(train_x)\n",
    "                dev_acc = accuracy(output_dev, dev_y)\n",
    "                train_acc = accuracy(output, train_y)\n",
    "                print(f\"Validation accuracy on batch {epoch}: {dev_acc}\")\n",
    "                print(f\"Training accuracy on batch {epoch}: {train_acc}\")\n",
    "        \n",
    "    torch.save(non_linear_classifier, \"non_linear_POS_GPT\")\n",
    "    \n",
    "    return non_linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m non_linear_classifier \u001B[38;5;241m=\u001B[39m pos_probe_non_linear(\u001B[43mtrain_x\u001B[49m, train_y, dev_x, dev_y)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "non_linear_classifier = pos_probe_non_linear(train_x, train_y, dev_x, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'non_linear_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mnon_linear_classifier\u001B[49m\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m      2\u001B[0m output_test \u001B[38;5;241m=\u001B[39m non_linear_classifier(test_x)\n\u001B[1;32m      3\u001B[0m test_acc \u001B[38;5;241m=\u001B[39m accuracy(output_test, test_y)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'non_linear_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "non_linear_classifier.eval()\n",
    "output_test = non_linear_classifier(test_x)\n",
    "test_acc = accuracy(output_test, test_y)\n",
    "print(\"The test accuracy fir the non_linear classifier is\",test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'non_linear_POS_GPT.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# ud_parses = parse_corpus('data/en_ewt-ud-train.conllu')\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# train_pos, train_vocab = fetch_pos_tags(ud_parses)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#     pos_vocab=train_vocab\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m non_linear_classifier \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnon_linear_POS_GPT.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m non_linear_classifier\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     14\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m torchmetrics\u001B[38;5;241m.\u001B[39mAccuracy()\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/serialization.py:231\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 231\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/PycharmProjects/nlp2-probing-lms/venv/lib/python3.8/site-packages/torch/serialization.py:212\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 212\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'non_linear_POS_GPT.pt'"
     ]
    }
   ],
   "source": [
    "# ud_parses = parse_corpus('data/en_ewt-ud-train.conllu')\n",
    "\n",
    "# train_pos, train_vocab = fetch_pos_tags(ud_parses)\n",
    "\n",
    "# test_x, test_y, _ = create_data(\n",
    "#     os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     pos_vocab=train_vocab\n",
    "# )\n",
    "\n",
    "non_linear_classifier = torch.load(\"non_linear_POS_GPT.pt\", map_location=torch.device('cpu'))\n",
    "non_linear_classifier.eval()\n",
    "accuracy = torchmetrics.Accuracy()\n",
    "output_test = non_linear_classifier(test_x)\n",
    "test_acc = accuracy(output_test, test_y)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Trees\n",
    "\n",
    "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly. I have provided code that transforms a `TokenTree` to a `Tree` in `ete3` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
    "\n",
    "def rec_tokentree_to_nltk(tokentree):\n",
    "    token = tokentree.token[\"form\"]\n",
    "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
    "\n",
    "    return tree_str\n",
    "\n",
    "\n",
    "def tokentree_to_nltk(tokentree):\n",
    "    from nltk import Tree as NLTKTree\n",
    "\n",
    "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
    "\n",
    "    return NLTKTree.fromstring(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install ete3\n",
    "from ete3 import Tree as EteTree\n",
    "\n",
    "\n",
    "class FancyTree(EteTree):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, format=1, **kwargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.get_ascii(show_internal=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def rec_tokentree_to_ete(tokentree):\n",
    "    idx = str(tokentree.token[\"id\"])\n",
    "    children = tokentree.children\n",
    "    if children:\n",
    "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
    "    else:\n",
    "        return idx\n",
    "    \n",
    "def tokentree_to_ete(tokentree):\n",
    "    newick_str = rec_tokentree_to_ete(tokentree)\n",
    "\n",
    "    return FancyTree(f\"{newick_str};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   /-2\n",
      "  |\n",
      "  |--3\n",
      "  |\n",
      "  |--4\n",
      "  |\n",
      "  |   /6 /-5\n",
      "  |  |\n",
      "  |  |   /-9\n",
      "  |  |  |\n",
      "  |  |  |--10\n",
      "  |  |  |\n",
      "  |  |  |--11\n",
      "  |  |-8|\n",
      "  |  |  |--12\n",
      "  |-7|  |\n",
      "  |  |  |--13\n",
      "  |  |  |\n",
      "  |  |   \\15/-14\n",
      "-1|  |\n",
      "  |  |   /-16\n",
      "  |  |  |\n",
      "  |  |  |--17\n",
      "  |  |  |\n",
      "  |   \\18   /-19\n",
      "  |     |  |\n",
      "  |     |  |--20\n",
      "  |     |  |\n",
      "  |     |  |-23/-22\n",
      "  |      \\21\n",
      "  |        |--24\n",
      "  |        |\n",
      "  |        |   /-25\n",
      "  |        |  |\n",
      "  |         \\28--26\n",
      "  |           |\n",
      "  |            \\-27\n",
      "  |\n",
      "   \\-29\n"
     ]
    }
   ],
   "source": [
    "# Let's check if it works!\n",
    "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
    "\n",
    "def parse_corpus(filename):\n",
    "    from conllu import parse_incr\n",
    "\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses\n",
    "\n",
    "corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "item = corpus[0]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see we label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
    "\n",
    "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances\n",
    "\n",
    "We will store all these distances in a `torch.Tensor`.\n",
    "\n",
    "Please fill in the gap in the following method. I recommend you to have a good look at Hewitt's blog post  about these node distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_gold_distances(corpus):\n",
    "    all_distances = []\n",
    "    sen_length = [len(i) for i in corpus]\n",
    "    max_length = max(sen_length)\n",
    "\n",
    "    for item in (corpus):\n",
    "        tokentree = item.to_tree()\n",
    "        ete_tree = tokentree_to_ete(tokentree)\n",
    "\n",
    "        sen_len = len(ete_tree.search_nodes())\n",
    "        distances = torch.full((max_length, max_length), -1)\n",
    "\n",
    "        for i in range(sen_len):\n",
    "            for j in range(sen_len):\n",
    "                node_i = ete_tree&f\"{i+1}\"\n",
    "                node_j = ete_tree&f\"{j+1}\"\n",
    "                distances[i][j] = node_i.get_distance(node_j)\n",
    "\n",
    "        all_distances.append(distances)\n",
    "\n",
    "    return all_distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
    "\n",
    "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). Fortunately for us, we can simply import a method from `scipy` that retrieves this MST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_mst(distances):\n",
    "    distances = torch.triu(distances).detach().numpy()\n",
    "    mst = minimum_spanning_tree(distances).toarray()\n",
    "    mst[mst>0] = 1.\n",
    "    \n",
    "    return mst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus.\n",
    "\n",
    "If your addition to the `create_gold_distances` method has been correct, you should be able to run the following snippet. This then shows you the original parse tree, the distances between the nodes, and the MST that is retrieved from these distances. Can you spot the edges in the MST matrix that correspond to the edges in the parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   /2 /-1\n",
      "  |\n",
      "  |--3\n",
      "  |\n",
      "  |--4\n",
      "  |\n",
      "  |   /-6\n",
      "  |  |\n",
      "-5|  |--7\n",
      "  |-8|\n",
      "  |  |   /-9\n",
      "  |  |  |\n",
      "  |   \\12--10\n",
      "  |     |\n",
      "  |      \\-11\n",
      "  |\n",
      "   \\-13 \n",
      "\n",
      "tensor([[0, 1, 3, 3, 2, 4, 4, 3, 5, 5, 5, 4, 3],\n",
      "        [1, 0, 2, 2, 1, 3, 3, 2, 4, 4, 4, 3, 2],\n",
      "        [3, 2, 0, 2, 1, 3, 3, 2, 4, 4, 4, 3, 2],\n",
      "        [3, 2, 2, 0, 1, 3, 3, 2, 4, 4, 4, 3, 2],\n",
      "        [2, 1, 1, 1, 0, 2, 2, 1, 3, 3, 3, 2, 1],\n",
      "        [4, 3, 3, 3, 2, 0, 2, 1, 3, 3, 3, 2, 3],\n",
      "        [4, 3, 3, 3, 2, 2, 0, 1, 3, 3, 3, 2, 3],\n",
      "        [3, 2, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 2],\n",
      "        [5, 4, 4, 4, 3, 3, 3, 2, 0, 2, 2, 1, 4],\n",
      "        [5, 4, 4, 4, 3, 3, 3, 2, 2, 0, 2, 1, 4],\n",
      "        [5, 4, 4, 4, 3, 3, 3, 2, 2, 2, 0, 1, 4],\n",
      "        [4, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 0, 3],\n",
      "        [3, 2, 2, 2, 1, 3, 3, 2, 4, 4, 4, 3, 0]]) \n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "item = corpus[5]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree, '\\n')\n",
    "\n",
    "gold_distance = create_gold_distances(corpus[5:6])[0]\n",
    "print(gold_distance, '\\n')\n",
    "\n",
    "mst = create_mst(gold_distance)\n",
    "print(mst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
    "\n",
    "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
    "\n",
    "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n",
    "\n",
    "You will write code that computes the UUAS score for a matrix of predicted distances, and the corresponding gold distances. I recommend you to split this up into 2 methods: 1 that retrieves the edges that are present in an MST matrix, and one general method that computes the UUAS score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def edges(mst):\n",
    "    edges = set()\n",
    "    n_nodes = mst.shape[0]\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(n_nodes):\n",
    "            if mst[i][j] != 0:\n",
    "                edges.add((i+1, j+1))\n",
    "\n",
    "    return edges\n",
    "\n",
    "def calc_uuas(pred_distances, gold_distances):\n",
    "    \n",
    "    gold_distances = gold_distances[gold_distances[0,:] != -1]\n",
    "    valid_cols = [col_idx for col_idx, col in enumerate(torch.split(gold_distances, 1, dim=1)) if not torch.all(col == -1)]\n",
    "    gold_distances = gold_distances[:, valid_cols]\n",
    "    sen_len = gold_distances.shape[0]\n",
    "    pred_distances = pred_distances[:sen_len,:sen_len]\n",
    "    gold_mst = create_mst(gold_distances)\n",
    "    pred_mst = create_mst(pred_distances)\n",
    "    pred_edges = edges(pred_mst)\n",
    "    gold_edges = edges(gold_mst)\n",
    "    pred_in_gold = len(pred_edges.intersection(gold_edges))\n",
    "    uuas = pred_in_gold/len(gold_distances)\n",
    "    \n",
    "    return uuas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Structural Probes\n",
    "\n",
    "We now have everything in place to start doing the actual exciting stuff: training our structural probe!\n",
    "    \n",
    "To make life easier for you, we will simply take the `torch` code for this probe from John Hewitt's repository. This allows you to focus on the training regime from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class StructuralProbe(nn.Module):\n",
    "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "    For a batch of sentences, computes all n^2 pairs of distances\n",
    "    for each sentence in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.probe_rank = rank\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "        \n",
    "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Computes all n^2 pairs of distances after projection\n",
    "        for each sentence in a batch.\n",
    "        Note that due to padding, some distances will be non-zero for pads.\n",
    "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "        Args:\n",
    "          batch: a batch of word representations of the shape\n",
    "            (batch_size, max_seq_len, representation_dim)\n",
    "        Returns:\n",
    "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        transformed = torch.matmul(batch, self.proj)\n",
    "        \n",
    "        batchlen, seqlen, rank = transformed.size()\n",
    "        \n",
    "        transformed = transformed.unsqueeze(2)\n",
    "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "        transposed = transformed.transpose(1,2)\n",
    "        \n",
    "        diffs = transformed - transposed\n",
    "        \n",
    "        squared_diffs = diffs.pow(2)\n",
    "        squared_distances = torch.sum(squared_diffs, -1)\n",
    "\n",
    "        return squared_distances\n",
    "\n",
    "    \n",
    "class L1DistanceLoss(nn.Module):\n",
    "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, label_batch, length_batch):\n",
    "        \"\"\" Computes L1 loss on distance matrices.\n",
    "        Ignores all entries where label_batch=-1\n",
    "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
    "        and then across the batch.\n",
    "        Args:\n",
    "          predictions: A pytorch batch of predicted distances\n",
    "          label_batch: A pytorch batch of true distances\n",
    "          length_batch: A pytorch batch of sentence lengths\n",
    "        Returns:\n",
    "          A tuple of:\n",
    "            batch_loss: average loss in the batch\n",
    "            total_sents: number of sentences in the batch\n",
    "        \"\"\"\n",
    "        labels_1s = (label_batch != -1).float()\n",
    "        predictions_masked = predictions * labels_1s\n",
    "        labels_masked = label_batch * labels_1s\n",
    "        total_sents = torch.sum((length_batch != 0)).float()\n",
    "        squared_lengths = length_batch.pow(2).float()\n",
    "\n",
    "        if total_sents > 0:\n",
    "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
    "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
    "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
    "        \n",
    "        else:\n",
    "            batch_loss = torch.tensor(0.0)\n",
    "        \n",
    "        return batch_loss, total_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_sen_reps_tree(ud_parses: List[TokenList], model, tokenizer, concat=False) -> Tensor:\n",
    "    representation_size = 768\n",
    "    out = []\n",
    "    sen_length = [len(i) for i in ud_parses]\n",
    "    max_length = max(sen_length)\n",
    "    \n",
    "    for sentence in tqdm(ud_parses):\n",
    "        j = 0\n",
    "        concat_dict = {}\n",
    "        token_list = []\n",
    "        space_after = False\n",
    "        for token in sentence:\n",
    "            \n",
    "            test_var = token[\"misc\"]\n",
    "            \n",
    "            if space_after:\n",
    "                token_return = tokenizer.encode(str(token), add_prefix_space=True)\n",
    "            else:\n",
    "                token_return = tokenizer.encode(str(token))\n",
    "            if test_var:\n",
    "                space_after = False\n",
    "            else:\n",
    "                space_after = True        \n",
    "            \n",
    "            len_i = len(token_return)\n",
    "            concat_dict[j] = [j+i for i in range(len_i)]\n",
    "            j += len_i\n",
    "            token_list += token_return\n",
    "        token_list = torch.LongTensor(token_list)\n",
    "        model.eval()\n",
    "        with torch.no_grad():          \n",
    "            out_sentence = model(input_ids = token_list, output_hidden_states=True)\n",
    "        out_sentence = out_sentence[\"hidden_states\"][-1].squeeze()        \n",
    "        \n",
    "        out_sent = torch.zeros(max_length, representation_size)\n",
    "        for i, key in enumerate(concat_dict):\n",
    "            out_sent[i] = torch.mean(out_sentence[concat_dict[key]], axis=0)\n",
    "        out.append(out_sent)\n",
    "    \n",
    "    out = torch.stack(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2/2 [00:00<00:00, 12.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0792,  0.3346, -0.1135,  ..., -0.2124,  0.1306, -0.1681],\n",
       "         [-0.3783,  0.0423, -0.0740,  ..., -0.0850,  0.4072, -0.2320],\n",
       "         [-0.4129,  0.4578, -0.7827,  ...,  0.0322, -0.5836,  0.4090],\n",
       "         ...,\n",
       "         [ 0.7832,  0.8050,  0.0341,  ..., -0.1987, -0.6397,  0.0914],\n",
       "         [ 0.5115,  0.4130, -0.8091,  ...,  0.1806, -0.1424,  0.2259],\n",
       "         [-0.8412,  0.2163,  0.0278,  ..., -0.1575, -0.1994, -0.0057]],\n",
       "\n",
       "        [[-0.1087,  0.3302, -0.0595,  ..., -0.1555,  0.1397, -0.1980],\n",
       "         [ 0.5315, -0.1347,  0.6371,  ...,  0.4578,  0.1667,  0.2503],\n",
       "         [ 0.3531,  0.0395, -0.6217,  ..., -0.0975,  0.2088,  0.0475],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sen_reps_tree(ud_parses[:2], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fetch_sen_reps_lstm_tree(ud_parses: List[TokenList], model, tokenizer) -> Tensor:\n",
    "    representation_size = 650\n",
    "    out = []\n",
    "    sen_length = [len(i) for i in ud_parses]\n",
    "    max_length = max(sen_length)\n",
    "    for sentence in ud_parses:\n",
    "        sent = torch.zeros((len(sentence), 1))\n",
    "        for i, token in enumerate(sentence):\n",
    "            sent[i] = tokenizer[str(token)]\n",
    "        sent = sent.long()\n",
    "        hidden = model.init_hidden(len(sentence))\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out_sentence = model(sent, hidden)\n",
    "        num_pad = max_length - len(out_sentence)\n",
    "        padding = torch.zeros((num_pad, representation_size))\n",
    "        out_sentence = torch.concat([out_sentence.squeeze(1), padding])\n",
    "\n",
    "        out.append(out_sentence)\n",
    "    out = torch.stack(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.7228e-04, -2.4058e-03,  2.1826e-02,  ..., -5.8634e-04,\n",
       "           2.1452e-03, -2.1771e-01],\n",
       "         [ 3.2399e-02,  2.3782e-02, -3.3276e-02,  ...,  1.4714e-02,\n",
       "           3.7484e-03, -3.8452e-02],\n",
       "         [ 1.1162e-01, -8.1289e-03,  1.6440e-03,  ..., -3.2698e-03,\n",
       "           8.8940e-03,  5.0699e-02],\n",
       "         ...,\n",
       "         [ 5.7555e-02, -1.4154e-02,  4.4750e-02,  ...,  7.0677e-02,\n",
       "           7.7484e-02,  3.4309e-02],\n",
       "         [-5.1764e-01, -5.5001e-03, -1.8755e-03,  ..., -5.3105e-03,\n",
       "           4.1017e-02,  5.8204e-01],\n",
       "         [-1.6005e-07, -3.6095e-09, -1.2251e-04,  ..., -1.5033e-09,\n",
       "           5.2312e-05,  6.5840e-05]],\n",
       "\n",
       "        [[ 2.2391e-02,  2.1226e-02,  1.2062e-03,  ...,  5.2355e-04,\n",
       "          -2.0393e-02,  3.0530e-03],\n",
       "         [ 9.4271e-03,  3.6337e-02, -3.2095e-01,  ...,  2.2678e-02,\n",
       "          -1.3870e-02,  5.7361e-02],\n",
       "         [ 3.5154e-02,  2.1094e-03,  7.9648e-03,  ...,  3.4737e-03,\n",
       "           5.3086e-02,  1.9762e-02],\n",
       "         ...,\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_sen_reps_lstm_tree(ud_parses[:2], lstm, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I have provided a rough outline for the training regime that you can use. Note that the hyper parameters that I provide here only serve as an indication, but should be (briefly) explored by yourself.\n",
    "\n",
    "As can be seen in Hewitt's code above, there exists functionality in the probe to deal with batched input. It is up to you to use that: a (less efficient) method can still incorporate batches by doing multiple forward passes for a batch and computing the backward pass only once for the summed losses of all these forward passes. (_I know, this is not the way to go, but in the interest of time that is allowed ;-), the purpose of the assignment is writing a good paper after all_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "Similar to the `create_data` method of the previous notebook, I recommend you to use a method \n",
    "that initialises all the data of a corpus. Note that for your embeddings you can use the \n",
    "`fetch_sen_reps` method again. However, for the POS probe you concatenated all these representations into \n",
    "1 big tensor of shape (num_tokens_in_corpus, model_dim). \n",
    "\n",
    "The StructuralProbe expects its input to contain all the representations of 1 sentence, so I recommend you\n",
    "to update your `fetch_sen_reps` method in a way that it is easy to retrieve all the representations that \n",
    "correspond to a single sentence.\n",
    "''' \n",
    "\n",
    "def init_corpus_gpt(path, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps_tree(corpus, model, tokenizer, concat=concat)    \n",
    "    gold_distances = torch.stack(create_gold_distances(corpus))\n",
    "    \n",
    "    return embs, gold_distances\n",
    "\n",
    "def init_corpus_lstm(path, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps_lstm_tree(corpus, lstm, vocab)    \n",
    "    gold_distances = torch.stack(create_gold_distances(corpus))\n",
    "    \n",
    "    return embs, gold_distances\n",
    "\n",
    "\n",
    "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
    "# Feel free to alter the signature of this method.\n",
    "def evaluate_probe(probe, _data):\n",
    "    probe.eval()\n",
    "    x, y = _data\n",
    "    loss_function =  L1DistanceLoss()\n",
    "    loss_function.eval()\n",
    "    uuas_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = probe(x)\n",
    "        length_batch = torch.count_nonzero(x, dim=1)[:,0]\n",
    "        loss_score, _ = loss_function(output, y, length_batch)\n",
    "        for i in range(output.shape[0]):\n",
    "            uuas_list.append(calc_uuas(output[i,:,:], y[i,:,:]))\n",
    "        uuas_score = sum(uuas_list)/len(uuas_list)\n",
    "    \n",
    "    return loss_score, uuas_score\n",
    "\n",
    "\n",
    "# Feel free to alter the signature of this method.\n",
    "def train(_data):\n",
    "    emb_dim = 768\n",
    "    rank = 64\n",
    "    lr = 10e-4\n",
    "    batch_size = 24\n",
    "    epochs = 200\n",
    "\n",
    "    probe = StructuralProbe(emb_dim, rank)\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
    "    loss_function =  L1DistanceLoss()\n",
    "    x, y = _data\n",
    "    dev_losses = []\n",
    "    dev_uuass = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            x_batch, y_batch = x[i:i+batch_size], y[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = probe(x_batch)\n",
    "            length_batch = torch.count_nonzero(x_batch, dim=1)\n",
    "            batch_loss, _ = loss_function(output, y_batch, length_batch[:,0])\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        dev_loss, dev_uuas = evaluate_probe(probe, _dev_data)\n",
    "        dev_losses.append(dev_loss)\n",
    "        dev_uuass.append(dev_uuas)\n",
    "\n",
    "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
    "        #scheduler.step(dev_loss)  \n",
    "    \n",
    "        if epoch % 20 == 0 :\n",
    "            model.eval()\n",
    "\n",
    "            print(f\"Validation loss on batch {epoch}: {dev_loss}\")\n",
    "            print(f\"UUA on batch {epoch}: {dev_uuas}\")\n",
    "        \n",
    "    test_loss, test_uuas = evaluate_probe(probe, _test_data)\n",
    "        \n",
    "    return dev_losses, dev_uuass, test_loss, test_uuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 12543/12543 [15:23<00:00, 13.59it/s]\n",
      "100%|ââââââââââ| 12543/12543 [15:47<00:00, 13.24it/s]\n",
      " 42%|âââââ     | 5291/12543 [06:37<07:55, 15.26it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_data = init_corpus_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'))\n",
    "_dev_data = init_corpus_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'))\n",
    "_test_data = init_corpus_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_losses, dev_uuas, test_loss, test_uuas = train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(test_loss, test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = np.linspace(1, 200, 200)\n",
    "ax.plot(x, dev_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# _test_data_gpt = init_corpus_gpt(os.path.join('', 'data/en_ewt-ud-test.conllu'))\n",
    "# _test_data_lstm = init_corpus_lstm(os.path.join('', 'data/en_ewt-ud-test.conllu'))\n",
    "\n",
    "probe_gpt = torch.load(\"Tree_GPT.pt\", map_location=torch.device('cpu'))\n",
    "probe_lstm = torch.load(\"Tree_LSTM.pt\", map_location=torch.device('cpu'))\n",
    "probe_gpt.eval()\n",
    "probe_lstm.eval()\n",
    "test_loss, test_uuas_gpt = evaluate_probe(probe_gpt, _test_data_gpt)\n",
    "test_loss, test_uuas_lstm = evaluate_probe(probe_lstm, _test_data_lstm)\n",
    "\n",
    "print(\"The uuas score for the gpt model is\", test_uuas_gpt)\n",
    "print(\"The uuas score for the lstm model is\", test_uuas_lstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LaTeX trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For your report you might want to add some of those fancy dependency tree plots like those of Figure 2 in the Structural Probing paper. For that you can use the following code, that outputs the corresponding LaTeX markup.\n",
    "\n",
    "**N.B.**: for the latex tikz tree the first token in a sentence has index 1 (instead of 0), so take that into account with the predicted and gold edges that you pass to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_tikz(predicted_edges, gold_edges, words):\n",
    "    \"\"\" Turns edge sets on word (nodes) into tikz dependency LaTeX.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_edges : Set[Tuple[int, int]]\n",
    "        Set (or list) of edge tuples, as predicted by your probe.\n",
    "    gold_edges : Set[Tuple[int, int]]\n",
    "        Set (or list) of gold edge tuples, as obtained from the treebank.\n",
    "    words : List[str]\n",
    "        List of strings representing the tokens in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    string = \"\"\"\\\\begin{dependency}[hide label, edge unit distance=.5ex]\n",
    "    \\\\begin{deptext}[column sep=0.05cm]\n",
    "    \"\"\"\n",
    "\n",
    "    string += (\n",
    "        \"\\\\& \".join([x.replace(\"$\", \"\\$\").replace(\"&\", \"+\") for x in words])\n",
    "        + \" \\\\\\\\\\n\"\n",
    "    )\n",
    "    string += \"\\\\end{deptext}\" + \"\\n\"\n",
    "    for i_index, j_index in gold_edges:\n",
    "        string += \"\\\\depedge[-]{{{}}}{{{}}}{{{}}}\\n\".format(i_index, j_index, \".\")\n",
    "    for i_index, j_index in predicted_edges:\n",
    "        string += f\"\\\\depedge[-,edge style={{red!60!}}, edge below]{{{i_index}}}{{{j_index}}}{{.}}\\n\"\n",
    "    string += \"\\\\end{dependency}\\n\"\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Control tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we have the code to answer the research question: \" Can the faithfulness of the probing results be validated, using control tasks?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### POS Tags With Linear and Non-Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first check this for the POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FETCH POS LABELS\n",
    "import random\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
    "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
    "def fetch_pos_tags_control(ud_parses: List[TokenList], pos_vocab=None) -> Tensor:\n",
    "    pos_tags = list()\n",
    "    if pos_vocab:\n",
    "        pos_vocab = pos_vocab\n",
    "    else:\n",
    "        pos_vocab = dict()\n",
    "    for sentence in ud_parses:\n",
    "        for token in sentence:\n",
    "            if str(token) in pos_vocab.keys():\n",
    "                pos_tags.append(pos_vocab[str(token)])\n",
    "            else:\n",
    "                i = random.sample(range(17), 1)\n",
    "                pos_vocab[str(token)] = i\n",
    "                pos_tags.append(i)\n",
    "\n",
    "    pos_tags = torch.as_tensor(pos_tags)\n",
    "  \n",
    "    return pos_tags, pos_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n"
     ]
    }
   ],
   "source": [
    "pos_tags_control = fetch_pos_tags_control(ud_parses)\n",
    "print(torch.unique(pos_tags_control[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that creates control labels for the POS tagging task. The x data to train on\n",
    "# is the same as before\n",
    "\n",
    "def create_pos_tags_control(filename: str, lm, w2i, pos_vocab=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    \n",
    "    pos_tags, pos_vocab = fetch_pos_tags_control(ud_parses, pos_vocab=pos_vocab)\n",
    "    \n",
    "    return pos_tags, pos_vocab\n",
    "\n",
    "\n",
    "use_sample = True\n",
    "\n",
    "train_y_control, train_vocab_control = create_pos_tags_control(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    ")\n",
    "\n",
    "dev_y_control, _ = create_pos_tags_control(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "    pos_vocab=train_vocab_control\n",
    ")\n",
    "\n",
    "test_y_control, _ = create_pos_tags_control(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "    pos_vocab=train_vocab_control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier_control = pos_probe_linear(train_x, train_y_control, dev_x, dev_y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier_control.eval()\n",
    "output_test_control = classifier_control(test_x)\n",
    "test_acc_control = accuracy(output_test_control, test_y_control)\n",
    "print(\"The test accuracy on the control task is\",test_acc_control.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_linear_classifier = pos_probe_non_linear(train_x, train_y_control, dev_x, dev_y_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_linear_classifier_control.eval()\n",
    "non_linear_output_test_control = non_linear_classifier_control(test_x)\n",
    "non_linear_test_acc_control = accuracy(non_linear_output_test_control, test_y_control)\n",
    "print(\"The test accuracy on the control task for the non-linear classifier is\",non_linear_test_acc_control.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Structural Probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we check this for the strucural probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from ete3 import Tree\n",
    "\n",
    "def create_gold_distances_control(corpus):\n",
    "    all_distances = []\n",
    "    sen_length = [len(i) for i in corpus]\n",
    "    max_length = max(sen_length)\n",
    "\n",
    "    for item in (corpus):\n",
    "        tokentree = item.to_tree()\n",
    "        ete_tree = tokentree_to_ete(tokentree)\n",
    "\n",
    "        sen_len = len(ete_tree.search_nodes())\n",
    "        root = int(ete_tree.name)\n",
    "        ete_tree = Tree()\n",
    "        A = ete_tree.add_child(name=str(0))\n",
    "        B = ete_tree.add_child(name=str(sen_len-1))\n",
    "        distances = torch.full((max_length, max_length), -1)\n",
    "\n",
    "        for i in range(1,sen_len-1):\n",
    "            if i != root:\n",
    "                choice = random.sample(range(3),1)\n",
    "                if choice[0] == 0:\n",
    "                    A.add_child(name=str(i))\n",
    "                if choice[0] == 1:\n",
    "                    B.add_child(name=str(i))\n",
    "                if choice[0] == 2:\n",
    "                    ete_tree.add_child(name=str(i))\n",
    "\n",
    "        for i in range(sen_len):\n",
    "            for j in range(sen_len):\n",
    "                if i != root:\n",
    "                    node_i = ete_tree&f\"{i}\"\n",
    "                else:\n",
    "                    node_i = ete_tree\n",
    "                if j != root:\n",
    "                    node_j = ete_tree&f\"{j}\"\n",
    "                else:\n",
    "                    node_j = ete_tree\n",
    "                distances[i][j] = node_i.get_distance(node_j)\n",
    "\n",
    "        all_distances.append(distances)\n",
    "\n",
    "    return all_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 2],\n",
      "        [1, 0, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 3],\n",
      "        [1, 2, 0, 4, 2, 2, 2, 4, 2, 2, 4, 2, 3],\n",
      "        [3, 4, 4, 0, 4, 2, 4, 2, 4, 4, 2, 4, 1],\n",
      "        [1, 2, 2, 4, 0, 2, 2, 4, 2, 2, 4, 2, 3],\n",
      "        [1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1],\n",
      "        [1, 2, 2, 4, 2, 2, 0, 4, 2, 2, 4, 2, 3],\n",
      "        [3, 4, 4, 2, 4, 2, 4, 0, 4, 4, 2, 4, 1],\n",
      "        [1, 2, 2, 4, 2, 2, 2, 4, 0, 2, 4, 2, 3],\n",
      "        [1, 2, 2, 4, 2, 2, 2, 4, 2, 0, 4, 2, 3],\n",
      "        [3, 4, 4, 2, 4, 2, 4, 2, 4, 4, 0, 4, 1],\n",
      "        [1, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 0, 3],\n",
      "        [2, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 0]]) \n",
      "\n",
      "[[0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "gold_distance_control = create_gold_distances_control(corpus[5:6])[0]\n",
    "print(gold_distance_control, '\\n')\n",
    "mst = create_mst(gold_distance_control)\n",
    "print(mst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_corpus_control_gpt(path, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps_tree(corpus, model, tokenizer, concat=concat)    \n",
    "    gold_distances = torch.stack(create_gold_distances_control(corpus))\n",
    "    \n",
    "    return embs, gold_distances\n",
    "\n",
    "def init_corpus_control_lstm(path, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps_lstm_tree(corpus, lstm, vocab)    \n",
    "    gold_distances = torch.stack(create_gold_distances_control(corpus))\n",
    "    \n",
    "    return embs, gold_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data = init_corpus_control_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'), cutoff=300)\n",
    "_dev_data = init_corpus_control_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'), cutoff=100)\n",
    "_test_data = init_corpus_control_gpt(os.path.join('', 'data/en_ewt-ud-train.conllu'), cutoff=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_losses, dev_uuas, test_loss, test_uuas = train(train_data)\n",
    "print(test_loss, test_uuas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The uuas score for the gpt model is 0.2426014114122034\n",
      "The uuas score for the lstm model is 0.2690943426395613\n"
     ]
    }
   ],
   "source": [
    "# _test_data_gpt_control = init_corpus_control_gpt(os.path.join('', 'data/en_ewt-ud-test.conllu'))\n",
    "_test_data_lstm_control = init_corpus_control_lstm(os.path.join('', 'data/en_ewt-ud-test.conllu'))\n",
    "\n",
    "probe_gpt_control = torch.load(\"Tree_GPT_control.pt\", map_location=torch.device('cpu'))\n",
    "probe_lstm_control = torch.load(\"Tree_LSTM_control.pt\", map_location=torch.device('cpu'))\n",
    "probe_gpt_control.eval()\n",
    "probe_lstm_control.eval()\n",
    "test_loss_control, test_uuas_gpt_control = evaluate_probe(probe_gpt_control, _test_data_gpt_control)\n",
    "test_loss_control, test_uuas_lstm_control = evaluate_probe(probe_lstm_control, _test_data_lstm_control)\n",
    "\n",
    "print(\"The uuas score for the gpt model is\", test_uuas_gpt_control)\n",
    "print(\"The uuas score for the lstm model is\", test_uuas_lstm_control)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}